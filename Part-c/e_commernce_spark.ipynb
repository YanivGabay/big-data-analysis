{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v0Ka-SwEcHl",
        "outputId": "b332af8a-d768-4940-e359-cdea289bdade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [Connected to cloud.r-\r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [Connecting to ppa.lau\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [Connecting to ppa.lau\r                                                                                                    \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80)] [Waiting for heade\r                                                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.5.1-bin-hadoop3  spark-3.5.1-bin-hadoop3.tgz\n",
            "-rw-r--r-- 1 root root 382M Feb 15 11:39 spark-3.5.1-bin-hadoop3.tgz\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!ls\n",
        "\n",
        "# Check if Spark tarball already exists and download only if it doesn't\n",
        "spark_file = 'spark-3.5.1-bin-hadoop3.tgz'\n",
        "if not os.path.exists(spark_file):\n",
        "    !wget https://dlcdn.apache.org/spark/spark-3.5.1/{spark_file}\n",
        "\n",
        "# Ensure the file is present\n",
        "!ls -lh {spark_file}\n",
        "\n",
        "# Extract the Spark tarball\n",
        "!tar xzf {spark_file}\n",
        "\n",
        "# Install findspark\n",
        "!pip install findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "# Initialize Spark using findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Fake Data Generation\").getOrCreate()\n",
        "# Verify Spark is initialized\n",
        "print(spark.version)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "fake = Faker()\n",
        "from faker.providers import DynamicProvider\n",
        "\n",
        "products_category = DynamicProvider(\n",
        "     provider_name=\"categories\",\n",
        "     elements=[\"home appliances\" ,\"phones\",\"laptops\", \"clothing\" , \"pharmacy\" , \"garden\",\"kids toys\"],\n",
        ")\n",
        "fake.add_provider(products_category)\n",
        "def generate_data(num_records):\n",
        "    data = []\n",
        "    for _ in range(num_records):\n",
        "        data.append((\n",
        "            fake.random_number(digits=5),  # user_id\n",
        "            fake.random_number(digits=7),  # product_id\n",
        "            random.choice(['view', 'cart', 'purchase']),  # event_type using random.choice\n",
        "            fake.random_number(digits=3),  # price\n",
        "            fake.date_time_this_year(),  # event_time\n",
        "            fake.categories(),  # category_code (using custom provider\n",
        "            fake.company(),  # brand\n",
        "            fake.uuid4()  # user_session\n",
        "        ))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "ADKG9UOjNm-s"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate data\n",
        "data = generate_data(1000)\n",
        "\n",
        "# Define schema for the DataFrame\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"user_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"product_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"event_type\", T.StringType(), True),\n",
        "    T.StructField(\"price\", T.IntegerType(), True),\n",
        "    T.StructField(\"event_time\", T.TimestampType(), True),\n",
        "    T.StructField(\"category_code\", T.StringType(), True),\n",
        "    T.StructField(\"brand\", T.StringType(), True),\n",
        "    T.StructField(\"user_session\", T.StringType(), True)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "CzgCwj9jNrP3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVO7Kn-_YXMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create DataFrame from the data\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show(5)\n",
        "\n",
        "df.printSchema()\n",
        "# Register the DataFrame as a temporary view to run SQL queries\n",
        "df.createOrReplaceTempView(\"events\")\n",
        "\n",
        "# SQL query to count the number of each event type\n",
        "result = spark.sql(\"\"\"\n",
        "SELECT event_type, COUNT(*) as count\n",
        "FROM events\n",
        "GROUP BY event_type\n",
        "ORDER BY count DESC\n",
        "\"\"\")\n",
        "\n",
        "# Show the query results\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6pV5d_qNxDZ",
        "outputId": "239389bb-52f6-43f8-9732-bc9d6aa622aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+-----+--------------------+---------------+--------------------+--------------------+\n",
            "|user_id|product_id|event_type|price|          event_time|  category_code|               brand|        user_session|\n",
            "+-------+----------+----------+-----+--------------------+---------------+--------------------+--------------------+\n",
            "|  56005|   1601859|      cart|  169|2024-01-24 22:20:...|       clothing|Nicholson, Lewis ...|bd6d5702-e2d6-4da...|\n",
            "|  16637|   6240909|  purchase|  279|2024-03-12 01:48:...|       pharmacy|Villegas, Barber ...|ff8de1b8-29b4-446...|\n",
            "|  22559|   9397413|      view|   25|2024-05-13 03:47:...|       clothing|       Cooley-Garcia|091b86d9-4360-45c...|\n",
            "|  69823|   7683406|  purchase|  482|2024-02-22 21:07:...|        laptops|      Padilla-Nelson|1c5712a8-fc79-4b1...|\n",
            "|  49380|    447809|      view|  536|2024-03-23 16:03:...|home appliances|   Mitchell-Peterson|9153d5d6-cf45-48b...|\n",
            "+-------+----------+----------+-----+--------------------+---------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            "\n",
            "+----------+-----+\n",
            "|event_type|count|\n",
            "+----------+-----+\n",
            "|      view|  349|\n",
            "|      cart|  327|\n",
            "|  purchase|  324|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Window function query for Spark SQL\n",
        "rolling_purchases_query = \"\"\"\n",
        "WITH DailyPurchases AS (\n",
        "    SELECT\n",
        "        category_code,\n",
        "        DATE(event_time) AS event_date,\n",
        "        COUNT(*) AS daily_purchases\n",
        "    FROM events\n",
        "    WHERE event_type = 'purchase'\n",
        "    GROUP BY category_code, DATE(event_time)\n",
        "), RollingTotals AS (\n",
        "    SELECT\n",
        "        category_code,\n",
        "        event_date,\n",
        "        daily_purchases,\n",
        "        SUM(daily_purchases) OVER (PARTITION BY category_code ORDER BY event_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rolling_purchases,\n",
        "        AVG(daily_purchases) OVER (PARTITION BY category_code ORDER BY event_date ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS avg_last_7_days\n",
        "    FROM DailyPurchases\n",
        ")\n",
        "SELECT\n",
        "    *,\n",
        "    (daily_purchases - avg_last_7_days) AS diff_from_avg\n",
        "FROM RollingTotals\n",
        "ORDER BY rolling_purchases DESC\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "rolling_purchases_results = spark.sql(rolling_purchases_query)\n",
        "rolling_purchases_results.show()\n",
        "\n",
        "# Optionally, convert to Pandas DataFrame for visualization\n",
        "rolling_purchases_pd = rolling_purchases_results.toPandas()\n",
        "\n",
        "# Visualization can be done using Plotly for interactive graphs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOlGuHWbYYbp",
        "outputId": "d535adcf-4dfc-491f-9057-bffaef913a77"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+---------------+-----------------+---------------+-------------+\n",
            "|  category_code|event_date|daily_purchases|rolling_purchases|avg_last_7_days|diff_from_avg|\n",
            "+---------------+----------+---------------+-----------------+---------------+-------------+\n",
            "|         phones|2024-06-26|              1|               57|          1.125|       -0.125|\n",
            "|         phones|2024-06-23|              1|               56|          1.375|       -0.375|\n",
            "|         phones|2024-06-18|              2|               55|            1.5|          0.5|\n",
            "|         phones|2024-06-14|              1|               53|          1.375|       -0.375|\n",
            "|         phones|2024-06-10|              1|               52|          1.375|       -0.375|\n",
            "|         phones|2024-06-05|              1|               51|          1.375|       -0.375|\n",
            "|         phones|2024-05-29|              1|               50|            1.5|         -0.5|\n",
            "|         garden|2024-06-26|              1|               49|          1.375|       -0.375|\n",
            "|         phones|2024-05-27|              1|               49|            1.5|         -0.5|\n",
            "|         garden|2024-06-15|              2|               48|          1.375|        0.625|\n",
            "|         phones|2024-05-26|              3|               48|            1.5|          1.5|\n",
            "|       clothing|2024-06-22|              1|               46|           1.25|        -0.25|\n",
            "|         garden|2024-06-12|              1|               46|           1.25|        -0.25|\n",
            "|      kids toys|2024-06-18|              2|               45|            1.5|          0.5|\n",
            "|         garden|2024-06-03|              2|               45|           1.25|         0.75|\n",
            "|       clothing|2024-06-18|              1|               45|           1.25|        -0.25|\n",
            "|home appliances|2024-06-15|              1|               45|            1.0|          0.0|\n",
            "|         phones|2024-05-22|              2|               45|           1.25|         0.75|\n",
            "|       clothing|2024-06-17|              1|               44|           1.25|        -0.25|\n",
            "|home appliances|2024-06-11|              1|               44|          1.125|       -0.125|\n",
            "+---------------+----------+---------------+-----------------+---------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Print DataFrame schema\n",
        "print(\"DataFrame Schema:\")\n",
        "rolling_purchases_results.printSchema()\n",
        "\n",
        "# Display summary statistics of the result DataFrame\n",
        "rolling_purchases_results.describe().show()\n",
        "\n",
        "# Show the top days with the highest rolling purchases\n",
        "rolling_purchases_results.sort(F.col(\"rolling_purchases\").desc()).show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxBfP3JJc6Xq",
        "outputId": "8804a981-026c-4a59-94fe-d0b39c4c3e51"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Schema:\n",
            "root\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- event_date: date (nullable = true)\n",
            " |-- daily_purchases: long (nullable = false)\n",
            " |-- rolling_purchases: long (nullable = true)\n",
            " |-- avg_last_7_days: double (nullable = true)\n",
            " |-- diff_from_avg: double (nullable = true)\n",
            "\n",
            "+-------+-------------+------------------+------------------+-------------------+-------------------+\n",
            "|summary|category_code|   daily_purchases| rolling_purchases|    avg_last_7_days|      diff_from_avg|\n",
            "+-------+-------------+------------------+------------------+-------------------+-------------------+\n",
            "|  count|          288|               288|               288|                288|                288|\n",
            "|   mean|         NULL|             1.125|          23.40625|  1.112070105820106|0.01292989417989418|\n",
            "| stddev|         NULL|0.3416500236732881|13.617403901786503|0.13502298258440176|  0.304596575413524|\n",
            "|    min|     clothing|                 1|                 1|                1.0|               -0.5|\n",
            "|    max|       phones|                 3|                57|                1.5|                1.5|\n",
            "+-------+-------------+------------------+------------------+-------------------+-------------------+\n",
            "\n",
            "+-------------+----------+---------------+-----------------+---------------+-------------+\n",
            "|category_code|event_date|daily_purchases|rolling_purchases|avg_last_7_days|diff_from_avg|\n",
            "+-------------+----------+---------------+-----------------+---------------+-------------+\n",
            "|       phones|2024-06-26|              1|               57|          1.125|       -0.125|\n",
            "|       phones|2024-06-23|              1|               56|          1.375|       -0.375|\n",
            "|       phones|2024-06-18|              2|               55|            1.5|          0.5|\n",
            "|       phones|2024-06-14|              1|               53|          1.375|       -0.375|\n",
            "|       phones|2024-06-10|              1|               52|          1.375|       -0.375|\n",
            "+-------------+----------+---------------+-----------------+---------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, register the DataFrame as a temp view if not already done\n",
        "rolling_purchases_results.createOrReplaceTempView(\"rolling_totals\")\n",
        "\n",
        "# SQL Query to find top 5 categories based on rolling purchases\n",
        "top_categories_query = \"\"\"\n",
        "SELECT category_code, SUM(rolling_purchases) as total_rolling_purchases\n",
        "FROM rolling_totals\n",
        "GROUP BY category_code\n",
        "ORDER BY total_rolling_purchases DESC\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "top_categories = spark.sql(top_categories_query)\n",
        "top_categories.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRZxewVulDI6",
        "outputId": "b5fdef97-04a6-47c8-ac62-bceab565b86b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------------+\n",
            "|  category_code|total_rolling_purchases|\n",
            "+---------------+-----------------------+\n",
            "|         phones|                   1384|\n",
            "|         garden|                   1053|\n",
            "|home appliances|                    999|\n",
            "|       clothing|                    966|\n",
            "|      kids toys|                    823|\n",
            "+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_purchases_pd\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to or create a SQLite database file\n",
        "conn = sqlite3.connect('ecommerce_data_spark.db')\n",
        "\n",
        "# Assuming `df` is your DataFrame containing the data\n",
        "rolling_purchases_pd.to_sql('sales_summary', conn, if_exists='replace', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('ecommerce_data_spark.db')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DZVDzOAowZB8",
        "outputId": "d429095c-4952-43aa-af86-1d361ec78924"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_167d32a2-fcdb-41e8-ab22-0c28b8c13773\", \"ecommerce_data_spark.db\", 20480)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}