{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v0Ka-SwEcHl",
        "outputId": "d2e4aabe-46c0-40be-ede3-729c943884a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.ubuntu.com (91.189.91\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.5.1-bin-hadoop3  spark-3.5.1-bin-hadoop3.tgz\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (26.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "-rw-r--r-- 1 root root 382M Feb 15 11:39 spark-3.5.1-bin-hadoop3.tgz\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!ls\n",
        "!pip install faker\n",
        "import os\n",
        "\n",
        "# Check if Spark tarball already exists and download only if it doesn't\n",
        "spark_file = 'spark-3.5.1-bin-hadoop3.tgz'\n",
        "if not os.path.exists(spark_file):\n",
        "    !wget https://dlcdn.apache.org/spark/spark-3.5.1/{spark_file}\n",
        "\n",
        "# Ensure the file is present\n",
        "!ls -lh {spark_file}\n",
        "\n",
        "# Extract the Spark tarball\n",
        "!tar xzf {spark_file}\n",
        "\n",
        "# Install findspark\n",
        "!pip install findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "# Initialize Spark using findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Fake Data Generation\").getOrCreate()\n",
        "# Verify Spark is initialized\n",
        "print(spark.version)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "fake = Faker()\n",
        "from faker.providers import DynamicProvider\n",
        "\n",
        "products_category = DynamicProvider(\n",
        "     provider_name=\"categories\",\n",
        "     elements=[\"home appliances\" ,\"phones\",\"laptops\", \"clothing\" , \"pharmacy\" , \"garden\",\"kids toys\"],\n",
        ")\n",
        "fake.add_provider(products_category)\n",
        "def generate_data(num_records):\n",
        "    data = []\n",
        "    for _ in range(num_records):\n",
        "        data.append((\n",
        "            fake.random_number(digits=5),  # user_id\n",
        "            fake.random_number(digits=7),  # product_id\n",
        "            random.choice(['view', 'cart', 'purchase']),  # event_type using random.choice\n",
        "            fake.random_number(digits=3),  # price\n",
        "            fake.date_time_this_year(),  # event_time\n",
        "            fake.categories(),  # category_code (using custom provider\n",
        "            fake.company(),  # brand\n",
        "            fake.uuid4()  # user_session\n",
        "        ))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "ADKG9UOjNm-s"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate data\n",
        "data = generate_data(100000)\n",
        "\n",
        "# Define schema for the DataFrame\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"user_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"product_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"event_type\", T.StringType(), True),\n",
        "    T.StructField(\"price\", T.IntegerType(), True),\n",
        "    T.StructField(\"event_time\", T.TimestampType(), True),\n",
        "    T.StructField(\"category_code\", T.StringType(), True),\n",
        "    T.StructField(\"brand\", T.StringType(), True),\n",
        "    T.StructField(\"user_session\", T.StringType(), True)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "CzgCwj9jNrP3"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVO7Kn-_YXMr"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create DataFrame from the data\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show(5)\n",
        "\n",
        "df.printSchema()\n",
        "# Register the DataFrame as a temporary view to run SQL queries\n",
        "df.createOrReplaceTempView(\"events\")\n",
        "\n",
        "# SQL query to count the number of each event type\n",
        "result = spark.sql(\"\"\"\n",
        "SELECT event_type, COUNT(*) as count\n",
        "FROM events\n",
        "GROUP BY event_type\n",
        "ORDER BY count DESC\n",
        "\"\"\")\n",
        "\n",
        "# Show the query results\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6pV5d_qNxDZ",
        "outputId": "8579525b-ae3e-4435-e4eb-be86e65e7b2e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+-----+--------------------+---------------+--------------------+--------------------+\n",
            "|user_id|product_id|event_type|price|          event_time|  category_code|               brand|        user_session|\n",
            "+-------+----------+----------+-----+--------------------+---------------+--------------------+--------------------+\n",
            "|  80784|   7944703|  purchase|  681|2024-03-27 10:15:...|         phones|         Simpson Ltd|67312624-4c55-4a2...|\n",
            "|  17710|   5451235|      cart|  491|2024-02-19 17:01:...|       pharmacy|        Phillips-Kim|68b74c67-130b-473...|\n",
            "|  56769|   5769330|      cart|    3|2024-05-19 13:08:...|         garden|           Salas Ltd|5d58b8bc-cffa-456...|\n",
            "|  98043|   8563157|      view|    4|2024-02-15 13:38:...|        laptops|         Scott-Booth|354a7a99-07ab-453...|\n",
            "|  76699|   8770319|      view|  705|2024-01-08 03:04:...|home appliances|Pham, Cox and Bishop|cf8cc0bf-6890-49b...|\n",
            "+-------+----------+----------+-----+--------------------+---------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            "\n",
            "+----------+-----+\n",
            "|event_type|count|\n",
            "+----------+-----+\n",
            "|  purchase|33508|\n",
            "|      cart|33379|\n",
            "|      view|33113|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Window function query for Spark SQL\n",
        "rolling_sales_summary_query = \"\"\"\n",
        "WITH DailySales AS (\n",
        "    SELECT\n",
        "        category_code,\n",
        "        DATE(event_time) AS event_date,\n",
        "        COUNT(*) AS daily_sales\n",
        "    FROM events\n",
        "    WHERE event_type = 'purchase'\n",
        "    GROUP BY category_code, DATE(event_time)\n",
        "), AvgSales AS (\n",
        "    SELECT\n",
        "        category_code,\n",
        "        event_date,\n",
        "        daily_sales,\n",
        "        AVG(daily_sales) OVER (PARTITION BY category_code ORDER BY event_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg_last_7_days\n",
        "    FROM DailySales\n",
        ")\n",
        "SELECT\n",
        "    category_code,\n",
        "    event_date,\n",
        "    daily_sales,\n",
        "    avg_last_7_days,\n",
        "    (daily_sales - avg_last_7_days) AS diff_from_avg\n",
        "FROM AvgSales\n",
        "ORDER BY event_date, category_code;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "sales_summary = spark.sql(rolling_sales_summary_query)\n",
        "sales_summary.show()\n",
        "\n",
        "# Optionally, convert to Pandas DataFrame for visualization\n",
        "sales_summary_pd = sales_summary.toPandas()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOlGuHWbYYbp",
        "outputId": "cd9cf08a-dece-4eb9-fd5a-7e123950b350"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+-----------+------------------+-------------------+\n",
            "|  category_code|event_date|daily_sales|   avg_last_7_days|      diff_from_avg|\n",
            "+---------------+----------+-----------+------------------+-------------------+\n",
            "|       clothing|2024-01-01|         33|              33.0|                0.0|\n",
            "|         garden|2024-01-01|         24|              24.0|                0.0|\n",
            "|home appliances|2024-01-01|         24|              24.0|                0.0|\n",
            "|      kids toys|2024-01-01|         24|              24.0|                0.0|\n",
            "|        laptops|2024-01-01|         23|              23.0|                0.0|\n",
            "|       pharmacy|2024-01-01|         15|              15.0|                0.0|\n",
            "|         phones|2024-01-01|         20|              20.0|                0.0|\n",
            "|       clothing|2024-01-02|         29|              31.0|               -2.0|\n",
            "|         garden|2024-01-02|         31|              27.5|                3.5|\n",
            "|home appliances|2024-01-02|         27|              25.5|                1.5|\n",
            "|      kids toys|2024-01-02|         30|              27.0|                3.0|\n",
            "|        laptops|2024-01-02|         28|              25.5|                2.5|\n",
            "|       pharmacy|2024-01-02|         30|              22.5|                7.5|\n",
            "|         phones|2024-01-02|         23|              21.5|                1.5|\n",
            "|       clothing|2024-01-03|         32|31.333333333333332| 0.6666666666666679|\n",
            "|         garden|2024-01-03|         20|              25.0|               -5.0|\n",
            "|home appliances|2024-01-03|         31|27.333333333333332|  3.666666666666668|\n",
            "|      kids toys|2024-01-03|         25|26.333333333333332|-1.3333333333333321|\n",
            "|        laptops|2024-01-03|         22|24.333333333333332| -2.333333333333332|\n",
            "|       pharmacy|2024-01-03|         30|              25.0|                5.0|\n",
            "+---------------+----------+-----------+------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Print DataFrame schema\n",
        "print(\"DataFrame Schema:\")\n",
        "sales_summary.printSchema()\n",
        "\n",
        "# Display summary statistics of the result DataFrame\n",
        "sales_summary.describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxBfP3JJc6Xq",
        "outputId": "8a27290b-72a4-423d-eba1-b41af362473f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Schema:\n",
            "root\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- event_date: date (nullable = true)\n",
            " |-- daily_sales: long (nullable = false)\n",
            " |-- avg_last_7_days: double (nullable = true)\n",
            " |-- diff_from_avg: double (nullable = true)\n",
            "\n",
            "+-------+-------------+------------------+------------------+--------------------+\n",
            "|summary|category_code|       daily_sales|   avg_last_7_days|       diff_from_avg|\n",
            "+-------+-------------+------------------+------------------+--------------------+\n",
            "|  count|         1267|              1267|              1267|                1267|\n",
            "|   mean|         NULL| 26.44672454617206| 26.49415191490958|-0.04742736873755022|\n",
            "| stddev|         NULL|5.2463147305388915|1.9855891418905245|   4.854147140514679|\n",
            "|    min|     clothing|                 7|              15.0| -16.285714285714285|\n",
            "|    max|       phones|                43|              33.0|                16.0|\n",
            "+-------+-------------+------------------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRZxewVulDI6"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to or create a SQLite database file\n",
        "conn = sqlite3.connect('ecommerce_data_spark.db')\n",
        "\n",
        "# Connect to SQLite and write data\n",
        "sales_summary_pd.to_sql('sales_summary', conn, if_exists='replace', index=False)\n",
        "\n",
        "# Clean-up and close the connection\n",
        "conn.close()\n",
        "from google.colab import files\n",
        "files.download('ecommerce_data_spark.db')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DZVDzOAowZB8",
        "outputId": "37368c58-7e66-4e95-e753-9366c51cf2e2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a06a481-d1dc-4c6a-ab19-5b0bbf8815c7\", \"ecommerce_data_spark.db\", 65536)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}